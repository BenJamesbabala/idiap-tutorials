{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiGPU training on CIFAR-10\n",
    "\n",
    "In this notebook we will train a wide residual network http://arxiv.org/abs/1605.07146 using multiple GPUs in a single machine. With minimal changes it achieves state-of-the-art on CIFAR-10, CIFAR-100 and SVHN.\n",
    "\n",
    "You will need cudnn to run this notebook, which can be downloaded from https://developer.nvidia.com/cudnn, and NVIDIA NCCL library for efficient multiGPU communications https://github.com/NVIDIA/nccl\n",
    "After, the following rocks are required:\n",
    "\n",
    "```\n",
    "luarocks install cudnn\n",
    "luarocks install optnet\n",
    "luarocks install nccl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cudnn = require 'cudnn'\n",
    "cunn = require 'cunn'\n",
    "optnet = require 'optnet'\n",
    "tablex = require 'pl.tablex'\n",
    "optim = require 'optim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = {\n",
    "  dataset = 'cifar10_original.t7',\n",
    "  num_classes = 10,\n",
    "  save = 'logs',\n",
    "  batchSize = 128,\n",
    "  learningRate = 0.1,\n",
    "  learningRateDecay = 0,\n",
    "  learningRateDecayRatio = 0.2,\n",
    "  weightDecay = 0.0005,\n",
    "  dampening = 0,\n",
    "  momentum = 0.9,\n",
    "  epoch_step = \"80\",\n",
    "  max_epoch = 300,\n",
    "  model = 'nin',\n",
    "  optimMethod = 'sgd',\n",
    "  init_value = 10,\n",
    "  depth = 22,\n",
    "  width = 4,\n",
    "  shortcutType = 'A',\n",
    "  nesterov = false,\n",
    "  imageSize = 32,\n",
    "  optnet_optimize = true,\n",
    "  nGPU = 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local nn = require 'nn'\n",
    "\n",
    "local function createModel(opt)\n",
    "   local depth = opt.depth\n",
    "   local width = opt.width\n",
    "   \n",
    "   local function wide_basic(nInputPlane, nOutputPlane, stride)\n",
    "      local nBottleneckPlane = nOutputPlane\n",
    "\n",
    "      local block = nn.Sequential()\n",
    "      local convs = nn.Sequential()     \n",
    "\n",
    "      local module = nInputPlane == nOutputPlane and convs or block\n",
    "      module:add(nn.SpatialBatchNormalization(nInputPlane))\n",
    "      module:add(nn.ReLU(true))\n",
    "      convs:add(nn.SpatialConvolution(nInputPlane,nBottleneckPlane,3,3,stride,stride,1,1):noBias())\n",
    "      convs:add(nn.SpatialBatchNormalization(nBottleneckPlane))\n",
    "      convs:add(nn.ReLU(true))\n",
    "      convs:add(nn.SpatialConvolution(nBottleneckPlane,nBottleneckPlane,3,3,1,1,1,1):noBias())\n",
    "      \n",
    "      local shortcut = nInputPlane == nOutputPlane and\n",
    "         nn.Identity() or\n",
    "         nn.SpatialConvolution(nInputPlane,nOutputPlane,1,1,stride,stride,0,0):noBias()\n",
    "     \n",
    "      return block\n",
    "         :add(nn.ConcatTable()\n",
    "            :add(convs)\n",
    "            :add(shortcut))\n",
    "         :add(nn.CAddTable(true))\n",
    "   end\n",
    "\n",
    "   -- Stacking Residual Units on the same stage\n",
    "   local function layer(block, nInputPlane, nOutputPlane, count, stride)\n",
    "      local s = nn.Sequential()\n",
    "      s:add(block(nInputPlane, nOutputPlane, stride))\n",
    "      for i=2,count do\n",
    "         s:add(block(nOutputPlane, nOutputPlane, 1))\n",
    "      end\n",
    "      return s\n",
    "   end\n",
    "\n",
    "   local model = nn.Sequential()\n",
    "   do\n",
    "      assert((depth - 4) % 6 == 0, 'depth should be 6n+4')\n",
    "      local n = (depth - 4) / 6\n",
    "      local k = width\n",
    "      local nStages = torch.Tensor{16, 16*k, 32*k, 64*k}\n",
    "\n",
    "      model:add(nn.SpatialConvolution(3,nStages[1],3,3,1,1,1,1)) -- one conv at the beginning (spatial size: 32x32)\n",
    "      model:add(layer(wide_basic, nStages[1], nStages[2], n, 1)) -- Stage 1 (spatial size: 32x32)\n",
    "      model:add(layer(wide_basic, nStages[2], nStages[3], n, 2)) -- Stage 2 (spatial size: 16x16)\n",
    "      model:add(layer(wide_basic, nStages[3], nStages[4], n, 2)) -- Stage 3 (spatial size: 8x8)\n",
    "      model:add(nn.SpatialBatchNormalization(nStages[4]))\n",
    "      model:add(nn.ReLU(true))\n",
    "      model:add(nn.SpatialAveragePooling(8, 8, 1, 1))\n",
    "      model:add(nn.View(nStages[4]):setNumInputDims(3))\n",
    "      model:add(nn.Linear(nStages[4], opt.num_classes))\n",
    "   end\n",
    "\n",
    "    local function MSRinit(model)\n",
    "       for k,v in pairs(model:findModules('nn.SpatialConvolution')) do\n",
    "          local n = v.kW*v.kH*v.nOutputPlane\n",
    "          v.weight:normal(0,math.sqrt(2/n))\n",
    "          if v.bias then v.bias:zero() end\n",
    "       end\n",
    "    end\n",
    "\n",
    "    local function FCinit(model)\n",
    "       for k,v in pairs(model:findModules'nn.Linear') do\n",
    "         v.bias:zero()\n",
    "       end\n",
    "    end\n",
    "    \n",
    "   MSRinit(model)\n",
    "   FCinit(model)\n",
    "\n",
    "   return model\n",
    "end\n",
    "\n",
    "-- define base network that will be used inside DataParallelTable\n",
    "local net = createModel(opt)\n",
    "cudnn.convert(net, cudnn):cuda()\n",
    "\n",
    "-- utilize optnet to reduce memory usage\n",
    "local sample_input = torch.randn(8,3,32,32):cuda()\n",
    "optnet.optimizeMemory(net, sample_input, {inplace = false, mode = 'training'})\n",
    "cudnn.benchmark = true\n",
    "\n",
    "model = net\n",
    "if opt.nGPU > 1 then\n",
    "    -- this will wrap our initial network into data parallel module\n",
    "    -- that will take care around multi-GPU communications in an efficient\n",
    "    -- manner, and will launch CUDA kernels in parallel to reduce overhead on\n",
    "    -- kernel launches\n",
    "    model = nn.DataParallelTable(1, true, true)\n",
    "         :add(model, gpus)\n",
    "         :threads(function()\n",
    "            local cudnn = require 'cudnn'\n",
    "            cudnn.benchmark = true\n",
    "         end)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "provider = torch.load(opt.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- don't call this more than once\n",
    "parameters,gradParameters = model:getParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local criterion = nn.CrossEntropyCriterion():cuda()\n",
    "\n",
    "-- a-la autograd\n",
    "local f = function(inputs, targets)\n",
    "   model:forward(inputs)\n",
    "   local loss = criterion:forward(model.output, targets)\n",
    "   local df_do = criterion:backward(model.output, targets)\n",
    "   model:backward(inputs, df_do)\n",
    "   return loss\n",
    "end\n",
    "\n",
    "local optimState = tablex.deepcopy(opt)\n",
    "\n",
    "\n",
    "function train()\n",
    "  model:training()\n",
    "\n",
    "  local inputs = torch.CudaTensor(opt.batchSize,3,32,32)\n",
    "  local targets = torch.CudaTensor(opt.batchSize)\n",
    "  local indices = torch.randperm(provider.trainData.data:size(1)):long():split(opt.batchSize)\n",
    "  -- remove last element so that all minibatches have equal size\n",
    "  indices[#indices] = nil\n",
    "\n",
    "  local loss = 0\n",
    "\n",
    "  for t,v in ipairs(indices) do\n",
    "    inputs:copy(provider.trainData.data:index(1,v))\n",
    "    targets:copy(provider.trainData.labels:index(1,v))\n",
    "\n",
    "    optim[opt.optimMethod](function(x)\n",
    "      if x ~= parameters then parameters:copy(x) end\n",
    "      model:zeroGradParameters()\n",
    "      loss = loss + f(inputs, targets)\n",
    "      return f,gradParameters\n",
    "    end, parameters, optimState)\n",
    "  end\n",
    "\n",
    "  return loss / #indices\n",
    "end\n",
    "\n",
    "function test()\n",
    "  model:evaluate()\n",
    "  local confusion = optim.ConfusionMatrix(opt.num_classes)\n",
    "  local data_split = provider.testData.data:split(opt.batchSize,1)\n",
    "  local labels_split = provider.testData.labels:split(opt.batchSize,1)\n",
    "\n",
    "  for i,v in ipairs(data_split) do\n",
    "    confusion:batchAdd(model:forward(v), labels_split[i])\n",
    "  end\n",
    "\n",
    "  confusion:updateValids()\n",
    "  return confusion.totalValid * 100\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
